{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PySpark\n",
    "### Chapter 4: DataFrames\n",
    "This notebook contains sample code from Chapter 4 of [Learning PySpark]() focusing on PySpark and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 09:50:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/09 09:50:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set JAVA_HOME to Java 17 (required for PySpark)\n",
    "# This ensures the notebook kernel uses the correct Java version\n",
    "java_home = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "if os.path.exists(java_home):\n",
    "    os.environ[\"JAVA_HOME\"] = java_home\n",
    "    os.environ[\"PATH\"] = f\"/opt/homebrew/opt/openjdk@17/bin:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Configure Spark to bind to localhost to avoid network binding issues\n",
    "conf = SparkConf().setAppName(\"LearningPySpark\") \\\n",
    "                  .set(\"spark.driver.host\", \"localhost\") \\\n",
    "                  .set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "\n",
    "# Create SparkSession and get SparkContext\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate your own DataFrame\n",
    "Instead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our own JSON data \n",
    "#   This way we don't have to access the file system yet.\n",
    "# Three double quotes create multi-line string literals in Python. Here you can use single \" quotes without escaping\n",
    "stringJSONRDD = sc.parallelize((\"\"\"\n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' \\n  { \"id\": \"123\",\\n    \"name\": \"Katie\",\\n    \"age\": 19,\\n    \"eyeColor\": \"brown\"\\n  }']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringJSONRDD.take(1)   # inspecting the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show() # inspecting the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")\n",
    "\n",
    "# purpose\n",
    "# Registers the DataFrame as a temporary view in Spark's SQL Catalog\n",
    "# Enables SQL queries on the DataFrame using spark.sql(...)\n",
    "# Temporary -- exists only for the current Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 22|\n",
      "| Simone| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT name, age FROM swimmersJSON WHERE age > 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (222894409.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m-- Query Data\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%sql \n",
    "-- Query Data\n",
    "select * from swimmersJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection\n",
    "Note that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Spark was able to determine infer the schema (when reviewing the schema using `.printSchema`).\n",
    "\n",
    "But what if we want to programmatically specify the schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n",
    "In this case, let's specify the schema for a `CSV` text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Generate our own CSV data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "\n",
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "#   Notice that we have redefined id as Long (instead of String)\n",
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Query the data\n",
    "select * from swimmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark engine to infer the schema via reflection.\n",
    "\n",
    "Additional Resources include:\n",
    "* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n",
    "* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####|| SparkSession\n",
    "\n",
    "Notice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`.\n",
    "* Entry point for reading data\n",
    "* Working with metadata\n",
    "* Configuration\n",
    "* Cluster resource management\n",
    "\n",
    "For more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with SQL\n",
    "With DataFrames, you can start writing your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL Query and return the data\n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the row count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count of rows in SQL\n",
    "spark.sql(\"select count(*) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can make use of `%sql` within the notebook cells of a Databricks notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Query all data\n",
    "select * from swimmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query id and age for swimmers with age = 22 via DataFrame API\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query id and age for swimmers with age = 22 in SQL\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Query id and age for swimmers with age = 22\n",
    "select id, age from swimmers where age = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "select name, eyeColor from swimmers where eyeColor like 'b%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with the DataFrame API\n",
    "With DataFrames, you can start writing your queries using the DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the values \n",
    "swimmers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Databricks `display` command to view the data easier\n",
    "display(swimmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of rows\n",
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the id, age where age = 22\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name, eyeColor where eyeColor like 'b%'\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Time Flight Performance\n",
    "Query flight departure delays by State and City by joining the departure delay and join to the airport codes (to identify state and city)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Queries\n",
    "Let's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set File Paths\n",
    "flightPerfFilePath = \"/Users/byeongminko/Documents/repos/Learning-PySpark/Chapter02/flight-data/departuredelays.csv\"\n",
    "airportsFilePath = \"/Users/byeongminko/Documents/repos/Learning-PySpark/Chapter02/flight-data/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')    # sets the column delimiter to tab\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# Cache the Departure Delays dataset \n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        a.City,\n",
    "        f.origin,\n",
    "        sum(f.delay) as Delays\n",
    "    from\n",
    "        FlightPerformance f\n",
    "    join airports a -- inner join \n",
    "        on a.IATA = f.origin\n",
    "    where a.State = 'WA'\n",
    "    group by a.City, f.origin\n",
    "    order by sum(f.delay) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "select a.City, f.origin, sum(f.delay) as Delays\n",
    "  from FlightPerformance f\n",
    "    join airports a\n",
    "      on a.IATA = f.origin\n",
    " where a.State = 'WA'\n",
    " group by a.City, f.origin\n",
    " order by sum(f.delay) desc\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Sum of Flight Delays by State (for the US)\n",
    "spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query Sum of Flight Delays by State (for the US)\n",
    "select a.State, sum(f.delay) as Delays\n",
    "  from FlightPerformance f\n",
    "    join airports a\n",
    "      on a.IATA = f.origin\n",
    " where a.Country = 'USA'\n",
    " group by a.State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query Sum of Flight Delays by State (for the US)\n",
    "select a.State, sum(f.delay) as Delays\n",
    "  from FlightPerformance f\n",
    "    join airports a\n",
    "      on a.IATA = f.origin\n",
    " where a.Country = 'USA'\n",
    " group by a.State "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, please refer to:\n",
    "* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n",
    "* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "name": "Ch4 - DataFrames",
  "notebookId": 4341522646494009
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
